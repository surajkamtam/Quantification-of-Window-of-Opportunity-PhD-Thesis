{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10070cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Reshape, Dropout, Attention, Concatenate, LayerNormalization, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ---------------------- LOS-specific Config ----------------------\n",
    "# LOS, a, window size, loss function\n",
    "LOS_CONFIG = {\n",
    "    \"A\": {\"a\": 0.1,  \"window_size\": 15, \"loss\": \"huber\"},\n",
    "    \"B\": {\"a\": -0.3, \"window_size\": 15, \"loss\": \"rmse\"},\n",
    "    \"C\": {\"a\": 0.2,  \"window_size\": 20, \"loss\": \"huber\"},\n",
    "    \"D\": {\"a\": -0.5, \"window_size\": 20, \"loss\": \"rmse\"},\n",
    "    \"E\": {\"a\": 1.5,  \"window_size\": 20, \"loss\": \"mae\"},\n",
    "}\n",
    "\n",
    "# ---------------------- Model Definition ----------------------\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def compute_residuals(y_true, y_pred, loss_type='rmse'):\n",
    "    if loss_type == 'rmse':\n",
    "        return np.sqrt(np.mean((y_true - y_pred) ** 2, axis=1))\n",
    "    elif loss_type == 'mse':\n",
    "        return np.mean((y_true - y_pred) ** 2, axis=1)\n",
    "    elif loss_type == 'mae':\n",
    "        return np.mean(np.abs(y_true - y_pred), axis=1)\n",
    "    elif loss_type == 'huber':\n",
    "        delta = 1.0\n",
    "        diff = y_true - y_pred\n",
    "        return np.mean(np.where(np.abs(diff) <= delta,\n",
    "                                0.5 * diff ** 2,\n",
    "                                delta * (np.abs(diff) - 0.5 * delta)), axis=1)\n",
    "    elif loss_type == 'hausdorff':\n",
    "        return np.array([\n",
    "            hausdorff_distance(np.expand_dims(y_true[i], axis=0), np.expand_dims(y_pred[i], axis=0))\n",
    "            for i in range(len(y_true))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
    "\n",
    "def hausdorff_distance(set1, set2):\n",
    "    dists = cdist(set1, set2, metric='euclidean')\n",
    "    forward = np.max(np.min(dists, axis=1))\n",
    "    backward = np.max(np.min(dists, axis=0))\n",
    "    return max(forward, backward)\n",
    "\n",
    "def build_prediction_model(input_shape, output_steps, n_features):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    attention = Attention()([x, x])  # Self-attention\n",
    "    x = Concatenate()([x, attention])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = GRU(64)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(output_steps * n_features)(x)\n",
    "    outputs = Reshape((output_steps, n_features))(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.00093286), loss='mse', metrics=['mae', f1])\n",
    "    return model\n",
    "\n",
    "def load_trained_model(input_shape, output_steps, n_features):\n",
    "    model = build_prediction_model(input_shape, output_steps, n_features)\n",
    "    model.load_weights(\"gru_epoch_100_10_5.weights.h5\")\n",
    "    return model\n",
    "\n",
    "# ---------------------- Helper Functions ----------------------\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    value_counts = np.bincount(data)\n",
    "    probabilities = value_counts / np.sum(value_counts)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def dynthreshold(vectime, vecvalue, window_size=15, a=0.2):\n",
    "    \"\"\"\n",
    "    Entropy-based dynamic threshold.\n",
    "    Parameters:\n",
    "        vectime: not used in entropy calc but kept for interface completeness.\n",
    "        vecvalue: series of values (will be rounded to int for entropy bins).\n",
    "        window_size: rolling window for entropy calculation.\n",
    "        a: scaling on std-dev to set threshold T1 = H_avg + a * sigma.\n",
    "    \"\"\"\n",
    "    packets_int = np.round(vecvalue).astype(int)\n",
    "    num_points = len(packets_int)\n",
    "    entropy_values = np.array([\n",
    "        calculate_entropy(packets_int[max(0, i - window_size):i + 1]) for i in range(num_points)\n",
    "    ])\n",
    "    H_avg = np.mean(entropy_values)\n",
    "    sigma = np.std(entropy_values)\n",
    "    T_1 = H_avg + a * sigma\n",
    "    return entropy_values, T_1, H_avg\n",
    "\n",
    "def test_single_sequence(model, input_sequence):\n",
    "    input_seq = np.expand_dims(input_sequence, axis=0)\n",
    "    prediction = model.predict(input_seq, verbose=0)[0]\n",
    "    return prediction\n",
    "\n",
    "def compute_ewma(series, alpha=0.2):\n",
    "    ewma = np.zeros_like(series)\n",
    "    ewma[0] = series[0]\n",
    "    for t in range(1, len(series)):\n",
    "        ewma[t] = alpha * series[t] + (1 - alpha) * ewma[t - 1]\n",
    "    return ewma\n",
    "\n",
    "# ---------------------- Main Processing ----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('All_LOS_DATA_decel_sequence_data_with_jerk.csv')\n",
    "\n",
    "    look_back = 10\n",
    "    look_forward = 5\n",
    "    n_features = 3\n",
    "    feature_names = ['Speed', 'Acceleration', 'Jerk']\n",
    "    target_feature = 'Jerk'\n",
    "\n",
    "    model = load_trained_model(\n",
    "        input_shape=(look_back, n_features),\n",
    "        output_steps=look_forward,\n",
    "        n_features=n_features\n",
    "    )\n",
    "\n",
    "    sequences = df[['LOS', 'DepartureTime', 'Iteration']].drop_duplicates()\n",
    "    metrics_summary = []\n",
    "\n",
    "    for idx, row in sequences.iterrows():\n",
    "        start_time = time.time()\n",
    "\n",
    "        los = row['LOS']\n",
    "        departure_time = row['DepartureTime']\n",
    "        iteration = row['Iteration']\n",
    "        print(los, departure_time, iteration)\n",
    "\n",
    "        # Per-LOS configuration (fallback provided just in case)\n",
    "        cfg = LOS_CONFIG.get(str(los), {\"a\": 0.2, \"window_size\": 15, \"loss\": \"rmse\"})\n",
    "        a_val = cfg[\"a\"]\n",
    "        win_size = cfg[\"window_size\"]\n",
    "        loss_type = cfg[\"loss\"]\n",
    "\n",
    "        # Extract this sequence\n",
    "        seq_df = df[(df['LOS'] == los) & (df['DepartureTime'] == departure_time) & (df['Iteration'] == iteration)]\n",
    "        if len(seq_df) < (look_back + look_forward):\n",
    "            continue\n",
    "\n",
    "        time_sequence = seq_df['Time'].values\n",
    "        full_features = seq_df[['Speed', 'Acceleration', 'Jerk']].values\n",
    "        full_labels = seq_df['Label'].values.astype(int)\n",
    "\n",
    "        all_predictions, all_actuals, all_times, all_labels = [], [], [], []\n",
    "\n",
    "        for i in range(0, len(full_features) - look_back - look_forward + 1, look_forward):\n",
    "            current_window = full_features[i:i+look_back]\n",
    "            actual_continuation = full_features[i+look_back:i+look_back+look_forward]\n",
    "            time_window = time_sequence[i+look_back:i+look_back+look_forward]\n",
    "            label_window = full_labels[i+look_back:i+look_back+look_forward]\n",
    "            prediction = test_single_sequence(model, current_window)\n",
    "            all_predictions.append(prediction)\n",
    "            all_actuals.append(actual_continuation)\n",
    "            all_times.append(time_window)\n",
    "            all_labels.append(label_window)\n",
    "\n",
    "        if not all_predictions:\n",
    "            continue\n",
    "\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "        all_actuals = np.concatenate(all_actuals)\n",
    "        all_times = np.concatenate(all_times)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        predicted_sequence = np.vstack([full_features[:look_back], all_predictions])\n",
    "        actual_sequence = np.vstack([full_features[:look_back], all_actuals])\n",
    "        time_plot = np.concatenate([time_sequence[:look_back], all_times])\n",
    "        test_labels = np.concatenate([full_labels[:look_back], all_labels])\n",
    "\n",
    "        # ---- LOS-specific residuals & anomaly scoring ----\n",
    "        feature_idx = feature_names.index(target_feature)\n",
    "\n",
    "        # Use only the target feature for residuals; shape to (N,1) to match compute_residuals' axis=1 behavior\n",
    "        y_true_seq = actual_sequence[:, feature_idx].reshape(-1, 1)\n",
    "        y_pred_seq = predicted_sequence[:, feature_idx].reshape(-1, 1)\n",
    "\n",
    "        residual_series = compute_residuals(y_true_seq, y_pred_seq, loss_type=loss_type)\n",
    "\n",
    "        # Smooth (discard the initial look_back to align with predictions like before)\n",
    "        smoothed_residuals = compute_ewma(residual_series[look_back:])\n",
    "\n",
    "        # Dynamic threshold with LOS-specific 'a' and entropy window\n",
    "        entropy_vals, T1, H_avg = dynthreshold(time_plot[look_back:], smoothed_residuals,\n",
    "                                               window_size=win_size, a=a_val)\n",
    "        anomalies = (entropy_vals > T1).astype(int)\n",
    "\n",
    "        # Align labels\n",
    "        y_true = test_labels[look_back:look_back+len(smoothed_residuals)]\n",
    "        y_pred = anomalies\n",
    "\n",
    "        # Metrics\n",
    "        TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, entropy_vals)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "\n",
    "        f1_val = f1_score(y_true, y_pred, zero_division=0)\n",
    "        prob_detection = (TP / (TP + FN)) * 100 if (TP + FN) > 0 else 0.0\n",
    "        false_alarm_rate = (FP / (FP + TN)) * 100 if (FP + TN) > 0 else 0.0\n",
    "        anomaly_probability = (np.sum(y_pred == 1) / len(y_pred)) if len(y_pred) > 0 else 0.0\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        metrics_summary.append({\n",
    "            'LOS': los,\n",
    "            'DepartureTime': departure_time,\n",
    "            'Iteration': iteration,\n",
    "            'LossType': loss_type,\n",
    "            'EntropyWin': win_size,\n",
    "            'Alpha_a': a_val,\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'FN': FN,\n",
    "            'TN': TN,\n",
    "            'PD': prob_detection,\n",
    "            'FAR': false_alarm_rate,\n",
    "            'F1': f1_val,\n",
    "            'AUC': auc,\n",
    "            'Anomaly_Prob': anomaly_probability,\n",
    "            'Time_sec': end_time - start_time\n",
    "        })\n",
    "\n",
    "        # ---- Progress summary (writes each pass so you can monitor long runs) ----\n",
    "        if metrics_summary:\n",
    "            metrics_df = pd.DataFrame(metrics_summary)\n",
    "            numeric_cols = metrics_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "            print(f\"\\n=== Summary Metrics for Feature: {target_feature} ===\")\n",
    "            print(f\"Window Configuration: look_back={look_back}, look_forward={look_forward}\")\n",
    "            print(f\"Total sequences analyzed: {len(metrics_df)}\\n\")\n",
    "\n",
    "            grouped = metrics_df.groupby('LOS')[numeric_cols]\n",
    "\n",
    "            median_metrics = grouped.median().round(4)\n",
    "            mean_metrics = grouped.mean().round(4)\n",
    "\n",
    "            print(\"Median Metrics by LOS:\")\n",
    "            print(median_metrics)\n",
    "\n",
    "            print(\"\\nMean Metrics by LOS:\")\n",
    "            print(mean_metrics)\n",
    "\n",
    "            # Save outputs (now include loss info in filename prefix)\n",
    "            median_metrics.to_csv(f\"median_metrics_{target_feature}.csv\")\n",
    "            mean_metrics.to_csv(f\"mean_metrics_{target_feature}.csv\")\n",
    "            metrics_df.to_csv(f\"metrics_summary_{target_feature}.csv\", index=False)\n",
    "        else:\n",
    "            print(f\"No metrics collected for feature: {target_feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6a9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
